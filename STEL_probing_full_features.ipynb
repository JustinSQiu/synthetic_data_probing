{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datadreamer import DataDreamer\n",
    "from datadreamer.llms import OpenAI\n",
    "from datadreamer.steps import DataFromPrompt, Embed, CosineSimilarity, concat, HFHubDataSource\n",
    "from datadreamer.embedders import SentenceTransformersEmbedder\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'Lexical Features' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    }
   ],
   "source": [
    "with DataDreamer(\"./output\"):\n",
    "    stel_dataset = HFHubDataSource(\n",
    "        \"Lexical Features\",\n",
    "        path=\"jjz5463/full_set_features_2.0\",\n",
    "        split=\"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(\n",
    "        dataset_pos, dataset_neg, model: str\n",
    "):\n",
    "    with DataDreamer(\"./output\"):\n",
    "        pos_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Positive Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_pos\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "        neg_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Negative Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_neg\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "    return pos_embedded_data, neg_embedded_data\n",
    "\n",
    "def convert_embeddings(pos_embedded_data, neg_embedded_data):\n",
    "    paired_embeddings = []\n",
    "    for i in range(len(pos_embedded_data.output) // 100):\n",
    "        pos_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][i * 100 : (i+1) * 100])\n",
    "        neg_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][i * 100 : (i+1) * 100])\n",
    "        paired = [(pos, neg) for pos, neg in zip(pos_embeddings, neg_embeddings)]\n",
    "        paired_embeddings.append(paired)\n",
    "    return paired_embeddings\n",
    "\n",
    "def compute_accuracy_STEL(paired_embeddings: list):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_neg, norm_alt_neg)\n",
    "            sim3 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            sim4 = np.dot(norm_anchor_neg, norm_alt_pos)\n",
    "            if math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) == math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) < math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)\n",
    "\n",
    "def compute_accuracy_STEL_or_content(paired_embeddings: list):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            norm_alt_neg = norm_anchor_neg\n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            if sim1 == sim2:\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif sim1 > sim2:\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)\n",
    "\n",
    "def STEL_benchmark(dataset_pos, dataset_neg, model, type='STEL'):\n",
    "    pos_embedded_data, neg_embedded_data = compute_embeddings(dataset_pos, dataset_neg, model)\n",
    "    paired_embeddings = convert_embeddings(pos_embedded_data, neg_embedded_data)\n",
    "    accuracies = []\n",
    "    for paired in paired_embeddings:\n",
    "        if type == 'STEL':\n",
    "            accuracies.append(compute_accuracy_STEL(paired))\n",
    "        elif type == 'STEL-or-content':\n",
    "            accuracies.append(compute_accuracy_STEL_or_content(paired))\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    return accuracies, avg_accuracy\n",
    "    \n",
    "\n",
    "def STEL_categories():\n",
    "    categories = []\n",
    "    for i in range(len(stel_dataset.output) // 100):\n",
    "        categories.append(stel_dataset.output['feature'][i * 100])\n",
    "    return categories\n",
    "\n",
    "def STEL_table(model, type='STEL'):\n",
    "    accuracies, avg_accuracy = STEL_benchmark(stel_dataset.output[\"positive\"], stel_dataset.output[\"negative\"], model, type)\n",
    "    accuracies.append(avg_accuracy)\n",
    "    categories = STEL_categories()\n",
    "    categories.append('average')\n",
    "    data = {\n",
    "        'Metric': categories,\n",
    "        f'{model} Embeddings': accuracies\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def merge_dfs(dfs):\n",
    "    for df in dfs:\n",
    "        df.set_index('Metric', inplace=True)\n",
    "    merged_df = pd.concat(dfs, axis=1)\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'sentence-transformers all-mpnet-base-v2 Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'sentence-transformers all-mpnet-base-v2 Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-uncased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-uncased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-multilingual-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-multilingual-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'distilbert distilbert-base-multilingual-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'distilbert distilbert-base-multilingual-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Metric                                                 |   AnnaWegmann/Style-Embedding Embeddings |   sentence-transformers/all-mpnet-base-v2 Embeddings |   google-bert/bert-base-uncased Embeddings |   google-bert/bert-base-cased Embeddings |   google-bert/bert-base-multilingual-cased Embeddings |   distilbert/distilbert-base-multilingual-cased Embeddings |\n",
      "|:-------------------------------------------------------|-----------------------------------------:|-----------------------------------------------------:|-------------------------------------------:|-----------------------------------------:|------------------------------------------------------:|-----------------------------------------------------------:|\n",
      "| Polite / Impolite                                      |                                0.234949  |                                           0.00040404 |                                0.0294949   |                               0.0529293  |                                           0.00989899  |                                                 0.00585859 |\n",
      "| With Humor / Without Humor                             |                                0.205455  |                                           0          |                                0.00767677  |                               0.0292929  |                                           0.0325253   |                                                 0.0214141  |\n",
      "| With sarcasm / Without sarcasm                         |                                0.274747  |                                           0          |                                0.00141414  |                               0.00545455 |                                           0.0206061   |                                                 0.0206061  |\n",
      "| With metaphor / Without metaphor                       |                                0.0743434 |                                           0          |                                0           |                               0          |                                           0.00242424  |                                                 0.00141414 |\n",
      "| Offensive / Non-Offensive                              |                                0.090303  |                                           0          |                                0.0109091   |                               0.0113131  |                                           0.0169697   |                                                 0.0115152  |\n",
      "| Positive / Negative                                    |                                0.0735354 |                                           0.00020202 |                                0           |                               0.00020202 |                                           0.000808081 |                                                 0.00242424 |\n",
      "| Active / Passive                                       |                                0.0214141 |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| Certain / Uncertain                                    |                                0.050303  |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| Self-focused / Inclusive-focused                       |                                0.0133333 |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| Self-focused / You-focused                             |                                0.0874747 |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| Self-focused / Audience-focused                        |                                0.0832323 |                                           0          |                                0.0010101   |                               0.00040404 |                                           0.00606061  |                                                 0.0113131  |\n",
      "| Self-focused / Third-person singular                   |                                0.0363636 |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| With personal pronouns / Less frequent pronouns        |                                0.06      |                                           0          |                                0.00141414  |                               0.00242424 |                                           0.00161616  |                                                 0.00585859 |\n",
      "| Present-focused / Future-focused                       |                                0.049697  |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| Present-focused / Past-focused                         |                                0.0282828 |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| Affective processes / Cognitive processes              |                                0.0313131 |                                           0          |                                0           |                               0          |                                           0.000808081 |                                                 0.00646465 |\n",
      "| Affective process / Perceptual process                 |                                0.0375758 |                                           0          |                                0.00020202  |                               0          |                                           0           |                                                 0.00282828 |\n",
      "| Cognitive process / Perceptual process                 |                                0.0151515 |                                           0          |                                0           |                               0          |                                           0           |                                                 0.00020202 |\n",
      "| With articles / Less frequent articles                 |                                0.0721212 |                                           0          |                                0.00040404  |                               0.00020202 |                                           0           |                                                 0          |\n",
      "| Fluent sentence / Disfluent sentence                   |                                0.370909  |                                           0          |                                0           |                               0.00585859 |                                           0.000808081 |                                                 0          |\n",
      "| With function words / Less frequent function words     |                                0.162222  |                                           0          |                                0           |                               0          |                                           0           |                                                 0.00020202 |\n",
      "| With common verbs / Less frequent common verbs         |                                0.0517172 |                                           0          |                                0.000606061 |                               0.00020202 |                                           0.000606061 |                                                 0.00121212 |\n",
      "| With pronouns / Less frequent pronouns                 |                                0.14404   |                                           0          |                                0.000808081 |                               0.00727273 |                                           0.00989899  |                                                 0.0153535  |\n",
      "| With prepositions / Less frequent prepositions         |                                0.19899   |                                           0          |                                0           |                               0          |                                           0           |                                                 0.00020202 |\n",
      "| With determiners / Less frequent determiners           |                                0.11596   |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| With conjunctions / Less frequent conjunctions         |                                0.292525  |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| With nominalizations / Without nominalizations         |                                0.134141  |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| Long average word length / Short average word length   |                                0.346263  |                                           0          |                                0.00929293  |                               0.0462626  |                                           0.0240404   |                                                 0.030303   |\n",
      "| With digits / Less frequent digits                     |                                0.0890909 |                                           0          |                                0.00141414  |                               0.00020202 |                                           0.0010101   |                                                 0.00020202 |\n",
      "| With uppercase letters / Without uppercase letters     |                                0.887677  |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| With frequent punctuation / Less Frequent punctuation  |                                0.672323  |                                           0          |                                0           |                               0          |                                           0.00020202  |                                                 0          |\n",
      "| Formal / Informal                                      |                                0.342222  |                                           0          |                                0.0117172   |                               0.0236364  |                                           0.0294949   |                                                 0.0206061  |\n",
      "| Complex / Simple                                       |                                0.22404   |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| With contractions / Without contractions               |                                0.256566  |                                           0          |                                0           |                               0          |                                           0           |                                                 0          |\n",
      "| With number substitution / Without number substitution |                                0.695556  |                                           0.0711111  |                                0.489091    |                               0.502626   |                                           0.308283    |                                                 0.380202   |\n",
      "| average                                                |                                0.186395  |                                           0.00204906 |                                0.0161558   |                               0.0196652  |                                           0.013316    |                                                 0.0153766  |\n"
     ]
    }
   ],
   "source": [
    "wegmann_table = STEL_table(\"AnnaWegmann/Style-Embedding\", type = 'STEL-or-content')\n",
    "mpnet_base_table = STEL_table(\"sentence-transformers/all-mpnet-base-v2\", type = 'STEL-or-content')\n",
    "bert_base_uncased_table = STEL_table(\"google-bert/bert-base-uncased\", type = 'STEL-or-content')\n",
    "bert_base_cased_table = STEL_table(\"google-bert/bert-base-cased\", type = 'STEL-or-content')\n",
    "bert_base_multilingual_table = STEL_table(\"google-bert/bert-base-multilingual-cased\", type = 'STEL-or-content')\n",
    "distilbert_base_multilingual_table = STEL_table(\"distilbert/distilbert-base-multilingual-cased\", type = 'STEL-or-content')\n",
    "dfs = [wegmann_table, mpnet_base_table, bert_base_uncased_table, bert_base_cased_table, bert_base_multilingual_table, distilbert_base_multilingual_table]\n",
    "merged_dfs = merge_dfs(dfs)\n",
    "print(merged_dfs.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
