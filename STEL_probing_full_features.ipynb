{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Justin Qiu/Desktop/NLP Research/Synthetic Data Probing/project_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datadreamer import DataDreamer\n",
    "from datadreamer.llms import OpenAI\n",
    "from datadreamer.steps import DataFromPrompt, Embed, CosineSimilarity, concat, HFHubDataSource\n",
    "from datadreamer.embedders import SentenceTransformersEmbedder\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'Lexical Features' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    }
   ],
   "source": [
    "with DataDreamer(\"./output\"):\n",
    "    stel_dataset = HFHubDataSource(\n",
    "        \"Lexical Features\",\n",
    "        path=\"StyleDistance/synthstel\",\n",
    "        split=\"test\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(\n",
    "        dataset_pos, dataset_neg, model: str\n",
    "):\n",
    "    with DataDreamer(\"./output\"):\n",
    "        pos_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Positive Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_pos\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "        neg_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Negative Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_neg\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "    return pos_embedded_data, neg_embedded_data\n",
    "\n",
    "def convert_embeddings(pos_embedded_data, neg_embedded_data):\n",
    "    paired_embeddings = []\n",
    "    for i in range(len(pos_embedded_data.output) // 10):\n",
    "        pos_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][i * 10 : (i+1) * 10])\n",
    "        neg_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][i * 10 : (i+1) * 10])\n",
    "        paired = [(pos, neg) for pos, neg in zip(pos_embeddings, neg_embeddings)]\n",
    "        paired_embeddings.append(paired)\n",
    "    return paired_embeddings\n",
    "\n",
    "def compute_accuracy_STEL(paired_embeddings: list):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_neg, norm_alt_neg)\n",
    "            sim3 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            sim4 = np.dot(norm_anchor_neg, norm_alt_pos)\n",
    "            if math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) == math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) < math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)\n",
    "\n",
    "def compute_accuracy_STEL_or_content(paired_embeddings: list):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            norm_alt_neg = norm_anchor_neg\n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            if sim1 == sim2:\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif sim1 > sim2:\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)\n",
    "\n",
    "def STEL_benchmark(dataset_pos, dataset_neg, model, type='STEL'):\n",
    "    pos_embedded_data, neg_embedded_data = compute_embeddings(dataset_pos, dataset_neg, model)\n",
    "    paired_embeddings = convert_embeddings(pos_embedded_data, neg_embedded_data)\n",
    "    accuracies = []\n",
    "    for paired in paired_embeddings:\n",
    "        if type == 'STEL':\n",
    "            accuracies.append(compute_accuracy_STEL(paired))\n",
    "        elif type == 'STEL-or-content':\n",
    "            accuracies.append(compute_accuracy_STEL_or_content(paired))\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    return accuracies, avg_accuracy\n",
    "    \n",
    "\n",
    "def STEL_categories():\n",
    "    categories = []\n",
    "    for i in range(len(stel_dataset.output) // 100):\n",
    "        categories.append(stel_dataset.output['feature'][i * 100])\n",
    "    return categories\n",
    "\n",
    "def STEL_table(model, type='STEL'):\n",
    "    accuracies, avg_accuracy = STEL_benchmark(stel_dataset.output['positive'], stel_dataset.output['negative'], model, type)\n",
    "    accuracies.append(avg_accuracy)\n",
    "    categories = STEL_categories()\n",
    "    categories.append('average')\n",
    "    data = {\n",
    "        'Metric': categories,\n",
    "        f'{model} Embeddings': accuracies\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def merge_dfs(dfs):\n",
    "    for df in dfs:\n",
    "        df.set_index('Metric', inplace=True)\n",
    "    merged_df = pd.concat(dfs, axis=1)\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'sentence-transformers all-mpnet-base-v2 Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'sentence-transformers all-mpnet-base-v2 Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-uncased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-uncased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-multilingual-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-multilingual-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'distilbert distilbert-base-multilingual-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'distilbert distilbert-base-multilingual-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'SynthSTEL styledistance Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'SynthSTEL styledistance Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'SynthSTEL styledistance_synthetic_only Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'SynthSTEL styledistance_synthetic_only Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Metric                                                 |   AnnaWegmann/Style-Embedding Embeddings |   sentence-transformers/all-mpnet-base-v2 Embeddings |   google-bert/bert-base-uncased Embeddings |   google-bert/bert-base-cased Embeddings |   google-bert/bert-base-multilingual-cased Embeddings |   distilbert/distilbert-base-multilingual-cased Embeddings |   SynthSTEL/styledistance Embeddings |   SynthSTEL/styledistance_synthetic_only Embeddings |\n",
      "|:-------------------------------------------------------|-----------------------------------------:|-----------------------------------------------------:|-------------------------------------------:|-----------------------------------------:|------------------------------------------------------:|-----------------------------------------------------------:|-------------------------------------:|----------------------------------------------------:|\n",
      "| Polite / Impolite                                      |                                 0.698586 |                                             0.853333 |                                   0.928889 |                                 0.941818 |                                              0.892929 |                                                   0.817374 |                             0.989697 |                                            0.646869 |\n",
      "| With Humor / Without Humor                             |                                 0.727879 |                                             0.746061 |                                   0.885657 |                                 0.881414 |                                              0.864444 |                                                   0.84303  |                             0.97899  |                                            0.707475 |\n",
      "| With sarcasm / Without sarcasm                         |                                 0.763232 |                                             0.718182 |                                   0.828283 |                                 0.78101  |                                              0.812929 |                                                   0.787071 |                             0.918586 |                                            0.643838 |\n",
      "| With metaphor / Without metaphor                       |                                 0.624646 |                                             0.814949 |                                   0.893333 |                                 0.881818 |                                              0.855152 |                                                   0.83596  |                             0.963636 |                                            0.785859 |\n",
      "| Offensive / Non-Offensive                              |                                 0.807879 |                                             0.913333 |                                   0.926869 |                                 0.938586 |                                              0.890707 |                                                   0.814949 |                             0.984848 |                                            0.723838 |\n",
      "| Positive / Negative                                    |                                 0.545051 |                                             0.967677 |                                   0.899394 |                                 0.891515 |                                              0.781616 |                                                   0.692727 |                             0.832323 |                                            0.605455 |\n",
      "| Active / Passive                                       |                                 0.643232 |                                             0.673737 |                                   0.871313 |                                 0.890707 |                                              0.876364 |                                                   0.936768 |                             0.913131 |                                            0.898788 |\n",
      "| Certain / Uncertain                                    |                                 0.564848 |                                             0.847475 |                                   0.870303 |                                 0.875152 |                                              0.775354 |                                                   0.776364 |                             0.856162 |                                            0.806263 |\n",
      "| Self-focused / Inclusive-focused                       |                                 0.653939 |                                             0.912727 |                                   0.995152 |                                 0.99596  |                                              0.995354 |                                                   0.996364 |                             0.81596  |                                            0.75798  |\n",
      "| Self-focused / You-focused                             |                                 0.94     |                                             0.649293 |                                   0.970101 |                                 0.972727 |                                              0.966869 |                                                   0.956566 |                             0.66     |                                            0.627677 |\n",
      "| Self-focused / Audience-focused                        |                                 0.991313 |                                             0.993131 |                                   0.996768 |                                 0.995758 |                                              0.998384 |                                                   0.999394 |                             0.769697 |                                            0.78303  |\n",
      "| Self-focused / Third-person singular                   |                                 0.542222 |                                             0.80303  |                                   0.828081 |                                 0.869899 |                                              0.858788 |                                                   0.853737 |                             0.769899 |                                            0.633737 |\n",
      "| With personal pronouns / Less frequent pronouns        |                                 0.543636 |                                             0.628485 |                                   0.838182 |                                 0.844848 |                                              0.865455 |                                                   0.90404  |                             0.819596 |                                            0.712323 |\n",
      "| Present-focused / Future-focused                       |                                 0.54     |                                             0.957576 |                                   0.947475 |                                 0.926869 |                                              0.918586 |                                                   0.926465 |                             0.75596  |                                            0.686061 |\n",
      "| Present-focused / Past-focused                         |                                 0.566263 |                                             0.979798 |                                   0.972929 |                                 0.960808 |                                              0.942828 |                                                   0.926465 |                             0.710909 |                                            0.634141 |\n",
      "| Affective processes / Cognitive processes              |                                 0.607273 |                                             0.867273 |                                   0.859192 |                                 0.83798  |                                              0.831515 |                                                   0.850303 |                             0.690303 |                                            0.737576 |\n",
      "| Affective process / Perceptual process                 |                                 0.592929 |                                             0.826667 |                                   0.831717 |                                 0.831313 |                                              0.808283 |                                                   0.805859 |                             0.847677 |                                            0.819394 |\n",
      "| Cognitive process / Perceptual process                 |                                 0.528889 |                                             0.722424 |                                   0.67899  |                                 0.704444 |                                              0.711313 |                                                   0.690101 |                             0.863232 |                                            0.687475 |\n",
      "| With articles / Less frequent articles                 |                                 0.74101  |                                             0.664242 |                                   0.966869 |                                 0.957778 |                                              0.949293 |                                                   0.952929 |                             0.866465 |                                            0.76202  |\n",
      "| Fluent sentence / Disfluent sentence                   |                                 0.833333 |                                             0.857778 |                                   0.988687 |                                 0.996566 |                                              0.997172 |                                                   0.998586 |                             0.889293 |                                            0.782424 |\n",
      "| With function words / Less frequent function words     |                                 0.627071 |                                             0.554141 |                                   0.779394 |                                 0.792929 |                                              0.761818 |                                                   0.775354 |                             0.769091 |                                            0.699596 |\n",
      "| With common verbs / Less frequent common verbs         |                                 0.668687 |                                             0.685051 |                                   0.905859 |                                 0.890303 |                                              0.813535 |                                                   0.847879 |                             0.773131 |                                            0.635556 |\n",
      "| With pronouns / Less frequent pronouns                 |                                 0.569293 |                                             0.601818 |                                   0.82404  |                                 0.846465 |                                              0.853939 |                                                   0.880404 |                             0.786061 |                                            0.618384 |\n",
      "| With prepositions / Less frequent prepositions         |                                 0.671717 |                                             0.534949 |                                   0.728687 |                                 0.780404 |                                              0.709697 |                                                   0.741818 |                             0.673333 |                                            0.680404 |\n",
      "| With determiners / Less frequent determiners           |                                 0.713131 |                                             0.620404 |                                   0.84404  |                                 0.88404  |                                              0.823232 |                                                   0.807273 |                             0.805253 |                                            0.684444 |\n",
      "| With conjunctions / Less frequent conjunctions         |                                 0.567273 |                                             0.548889 |                                   0.727071 |                                 0.804444 |                                              0.689899 |                                                   0.730707 |                             0.758788 |                                            0.709697 |\n",
      "| With nominalizations / Without nominalizations         |                                 0.758384 |                                             0.671919 |                                   0.827879 |                                 0.823434 |                                              0.798788 |                                                   0.842828 |                             0.67899  |                                            0.627475 |\n",
      "| Long average word length / Short average word length   |                                 0.856566 |                                             0.825253 |                                   0.993535 |                                 0.993737 |                                              0.966061 |                                                   0.987071 |                             0.976364 |                                            0.78202  |\n",
      "| With digits / Less frequent digits                     |                                 0.529293 |                                             0.719192 |                                   0.842424 |                                 0.838586 |                                              0.772323 |                                                   0.802424 |                             0.913333 |                                            0.620808 |\n",
      "| With uppercase letters / Without uppercase letters     |                                 1        |                                             0.5      |                                   0.5      |                                 0.98     |                                              0.987273 |                                                   0.987071 |                             1        |                                            1        |\n",
      "| With frequent punctuation / Less Frequent punctuation  |                                 0.903636 |                                             0.653333 |                                   0.867071 |                                 0.906263 |                                              0.883434 |                                                   0.919798 |                             0.673535 |                                            0.635152 |\n",
      "| Formal / Informal                                      |                                 0.852323 |                                             0.813333 |                                   0.973535 |                                 0.979192 |                                              0.971717 |                                                   0.954545 |                             0.979596 |                                            0.630101 |\n",
      "| Complex / Simple                                       |                                 0.593737 |                                             0.593131 |                                   0.740606 |                                 0.709495 |                                              0.648283 |                                                   0.61596  |                             0.68101  |                                            0.702424 |\n",
      "| With contractions / Without contractions               |                                 0.98303  |                                             0.890707 |                                   0.980606 |                                 0.978384 |                                              0.987879 |                                                   0.978586 |                             0.782222 |                                            0.667273 |\n",
      "| With number substitution / Without number substitution |                                 0.946869 |                                             0.953131 |                                   0.999798 |                                 1        |                                              0.997172 |                                                   0.992121 |                             0.971515 |                                            0.930101 |\n",
      "| average                                                |                                 0.705633 |                                             0.758926 |                                   0.871792 |                                 0.89099  |                                              0.864525 |                                                   0.863683 |                             0.83196  |                                            0.716162 |\n"
     ]
    }
   ],
   "source": [
    "tpe = 'STEL'\n",
    "\n",
    "wegmann_table = STEL_table(\"AnnaWegmann/Style-Embedding\", type = tpe)\n",
    "mpnet_base_table = STEL_table(\"sentence-transformers/all-mpnet-base-v2\", type = tpe)\n",
    "bert_base_uncased_table = STEL_table(\"google-bert/bert-base-uncased\", type = tpe)\n",
    "bert_base_cased_table = STEL_table(\"google-bert/bert-base-cased\", type = tpe)\n",
    "bert_base_multilingual_table = STEL_table(\"google-bert/bert-base-multilingual-cased\", type = tpe)\n",
    "distilbert_base_multilingual_table = STEL_table(\"distilbert/distilbert-base-multilingual-cased\", type = tpe)\n",
    "synth_stel_table = STEL_table(\"SynthSTEL/styledistance\", type = tpe)\n",
    "synth_stel_only_table = STEL_table(\"SynthSTEL/styledistance_synthetic_only\", type = tpe)\n",
    "dfs = [wegmann_table, mpnet_base_table, bert_base_uncased_table, bert_base_cased_table, bert_base_multilingual_table, distilbert_base_multilingual_table, synth_stel_table, synth_stel_only_table]\n",
    "merged_dfs = merge_dfs(dfs)\n",
    "print(merged_dfs.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'Lexical Features' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'FacebookAI roberta-base Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'FacebookAI roberta-base Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'SynthSTEL styledistance Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'SynthSTEL styledistance Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'SynthSTEL styledistance_synthetic_only Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'SynthSTEL styledistance_synthetic_only Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'StyleDistance styledistance_synthetic_only_ablation_hard Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'StyleDistance styledistance_synthetic_only_ablation_hard Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'StyleDistance styledistance_synthetic_only_ablation_easy Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'StyleDistance styledistance_synthetic_only_ablation_easy Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    }
   ],
   "source": [
    "from datadreamer import DataDreamer\n",
    "from datadreamer.llms import OpenAI\n",
    "from datadreamer.steps import DataFromPrompt, Embed, CosineSimilarity, concat, HFHubDataSource\n",
    "from datadreamer.embedders import SentenceTransformersEmbedder\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import tabulate\n",
    "\n",
    "NUM_ROWS_PER_CATEGORY = 10\n",
    "\n",
    "with DataDreamer(\"./output\"):\n",
    "    stel_dataset = HFHubDataSource(\n",
    "        \"Lexical Features\",\n",
    "        path=\"StyleDistance/synthstel\",\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_embeddings(\n",
    "        dataset_pos, dataset_neg, model: str\n",
    "):\n",
    "    with DataDreamer(\"./output\"):\n",
    "        pos_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Positive Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_pos\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "        neg_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Negative Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_neg\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "    return pos_embedded_data, neg_embedded_data\n",
    "\n",
    "def convert_embeddings(pos_embedded_data, neg_embedded_data):\n",
    "    paired_embeddings = []\n",
    "    for i in range(len(pos_embedded_data.output) // NUM_ROWS_PER_CATEGORY):\n",
    "        pos_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][i * NUM_ROWS_PER_CATEGORY : (i+1) * NUM_ROWS_PER_CATEGORY])\n",
    "        neg_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][i * NUM_ROWS_PER_CATEGORY : (i+1) * NUM_ROWS_PER_CATEGORY])\n",
    "        paired = [(pos, neg) for pos, neg in zip(pos_embeddings, neg_embeddings)]\n",
    "        paired_embeddings.append(paired)\n",
    "    return paired_embeddings\n",
    "\n",
    "def compute_accuracy_STEL(paired_embeddings: list):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_neg, norm_alt_neg)\n",
    "            sim3 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            sim4 = np.dot(norm_anchor_neg, norm_alt_pos)\n",
    "            if math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) == math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) < math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)\n",
    "\n",
    "def compute_accuracy_STEL_or_content(paired_embeddings: list):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            norm_alt_neg = norm_anchor_neg\n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            if sim1 == sim2:\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif sim1 > sim2:\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)\n",
    "\n",
    "def STEL_benchmark(dataset_pos, dataset_neg, model, type='STEL'):\n",
    "    pos_embedded_data, neg_embedded_data = compute_embeddings(dataset_pos, dataset_neg, model)\n",
    "    paired_embeddings = convert_embeddings(pos_embedded_data, neg_embedded_data)\n",
    "    accuracies = []\n",
    "    for paired in paired_embeddings:\n",
    "        if type == 'STEL':\n",
    "            accuracies.append(compute_accuracy_STEL(paired))\n",
    "        elif type == 'STEL-or-content':\n",
    "            accuracies.append(compute_accuracy_STEL_or_content(paired))\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    return accuracies, avg_accuracy\n",
    "    \n",
    "\n",
    "def STEL_categories():\n",
    "    categories = []\n",
    "    for i in range(len(stel_dataset.output) // NUM_ROWS_PER_CATEGORY):\n",
    "        categories.append(stel_dataset.output['feature'][i * NUM_ROWS_PER_CATEGORY])\n",
    "    return categories\n",
    "\n",
    "def STEL_table(model, type='STEL'):\n",
    "    accuracies, avg_accuracy = STEL_benchmark(stel_dataset.output['positive'], stel_dataset.output['negative'], model, type)\n",
    "    accuracies.append(avg_accuracy)\n",
    "    categories = STEL_categories()\n",
    "    categories.append('average')\n",
    "    data = {\n",
    "        'Metric': categories,\n",
    "        f'{model} Embeddings': accuracies\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def merge_dfs(dfs):\n",
    "    for df in dfs:\n",
    "        df.set_index('Metric', inplace=True)\n",
    "    merged_df = pd.concat(dfs, axis=1)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "tpe = 'STEL-or-content'\n",
    "\n",
    "models = ['AnnaWegmann/Style-Embedding', 'google-bert/bert-base-cased', 'FacebookAI/roberta-base', 'SynthSTEL/styledistance', 'SynthSTEL/styledistance_synthetic_only', 'StyleDistance/styledistance_synthetic_only_ablation_hard', 'StyleDistance/styledistance_synthetic_only_ablation_easy']\n",
    "tables = [STEL_table(model, type=tpe) for model in models]\n",
    "merged_dfs = merge_dfs(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merged_dfs.T\n",
    "final_df.to_excel('stel_or_content.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
