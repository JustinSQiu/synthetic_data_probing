{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datadreamer import DataDreamer\n",
    "from datadreamer.llms import OpenAI\n",
    "from datadreamer.steps import DataFromPrompt, Embed, CosineSimilarity, concat, HFHubDataSource\n",
    "from datadreamer.embedders import SentenceTransformersEmbedder\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "# from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'Lexical Features' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    }
   ],
   "source": [
    "with DataDreamer(\"./output\"):\n",
    "    stel_dataset = HFHubDataSource(\n",
    "        \"Lexical Features\",\n",
    "        path=\"jjz5463/probing_dataset_4.0\",\n",
    "        split=\"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(\n",
    "        dataset_pos, dataset_neg, model: str\n",
    "):\n",
    "    with DataDreamer(\"./output\"):\n",
    "        pos_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Positive Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_pos\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "        neg_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Negative Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_neg\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "    return pos_embedded_data, neg_embedded_data\n",
    "\n",
    "def convert_embeddings(pos_embedded_data, neg_embedded_data):\n",
    "    pos_formal_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][0:100])\n",
    "    neg_formal_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][0:100])\n",
    "    paired_formal_embeddings = [(pos, neg) for pos, neg in zip(pos_formal_embeddings, neg_formal_embeddings)]\n",
    "    pos_complex_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][100:200])\n",
    "    neg_complex_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][100:200])\n",
    "    paired_complex_embeddings = [(pos, neg) for pos, neg in zip(pos_complex_embeddings, neg_complex_embeddings)]\n",
    "    pos_contraction_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][200:300])\n",
    "    neg_contraction_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][200:300])\n",
    "    paired_contraction_embeddings = [(pos, neg) for pos, neg in zip(pos_contraction_embeddings, neg_contraction_embeddings)]\n",
    "    pos_number_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][300:400])\n",
    "    neg_number_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][300:400])\n",
    "    paired_number_embeddings = [(pos, neg) for pos, neg in zip(pos_number_embeddings, neg_number_embeddings)]\n",
    "    return paired_formal_embeddings, paired_complex_embeddings, paired_contraction_embeddings, paired_number_embeddings\n",
    "\n",
    "def compute_accuracy(paired_embeddings: list):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        # anchor_pos = anchor_pos.reshape(1, -1)\n",
    "        # anchor_neg = anchor_neg.reshape(1, -1)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            # alt_pos = alt_pos.reshape(1, -1)\n",
    "            # alt_neg = alt_neg.reshape(1, -1)\n",
    "            \n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_neg, norm_alt_neg)\n",
    "            sim3 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            sim4 = np.dot(norm_anchor_neg, norm_alt_pos)\n",
    "            # sim1 = cosine_similarity(anchor_pos, alt_pos)[0][0]\n",
    "            # sim2 = cosine_similarity(anchor_neg, alt_neg)[0][0]\n",
    "            # sim3 = cosine_similarity(anchor_pos, alt_neg)[0][0]\n",
    "            # sim4 = cosine_similarity(anchor_neg, alt_pos)[0][0]\n",
    "            if math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) == math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) < math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)\n",
    "\n",
    "def STEL_benchmark(dataset_pos, dataset_neg, model):\n",
    "    pos_embedded_data, neg_embedded_data = compute_embeddings(dataset_pos, dataset_neg, model)\n",
    "    paired_formal_embeddings, paired_complex_embeddings, paired_contraction_embeddings, paired_number_embeddings = convert_embeddings(pos_embedded_data, neg_embedded_data)\n",
    "    formal_accuracy = compute_accuracy(paired_formal_embeddings)\n",
    "    complex_accuracy = compute_accuracy(paired_complex_embeddings)\n",
    "    contraction_accuracy = compute_accuracy(paired_contraction_embeddings)\n",
    "    number_accuracy = compute_accuracy(paired_number_embeddings)\n",
    "    avg_accuracy = (formal_accuracy + complex_accuracy + contraction_accuracy + number_accuracy) / 4\n",
    "    return formal_accuracy, complex_accuracy, contraction_accuracy, number_accuracy, avg_accuracy\n",
    "\n",
    "def STEL_print(model):\n",
    "    formal_accuracy, complex_accuracy, contraction_accuracy, number_accuracy, avg_accuracy = STEL_benchmark(stel_dataset.output[\"positive\"], stel_dataset.output[\"negative\"], model)\n",
    "    print(f\"Formal Accuracy for {model} Embeddings: {formal_accuracy}\")\n",
    "    print(f\"Complex Accuracy for {model} Embeddings: {complex_accuracy}\")\n",
    "    print(f\"Contraction Accuracy for {model} Embeddings: {contraction_accuracy}\")\n",
    "    print(f\"Number Accuracy for {model} Embeddings: {number_accuracy}\")\n",
    "    print(f\"Average Accuracy for {model} Embeddings: {avg_accuracy}\")\n",
    "\n",
    "def STEL_table(model):\n",
    "    formal_accuracy, complex_accuracy, contraction_accuracy, number_accuracy, avg_accuracy = STEL_benchmark(stel_dataset.output[\"positive\"], stel_dataset.output[\"negative\"], model)\n",
    "    data = {\n",
    "        'Metric': ['Formal Accuracy', 'Complex Accuracy', 'Contraction Accuracy', 'Number Accuracy', 'Average Accuracy'],\n",
    "        f'{model} Embeddings': [formal_accuracy, complex_accuracy, contraction_accuracy, number_accuracy, avg_accuracy]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'sentence-transformers all-mpnet-base-v2 Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'sentence-transformers all-mpnet-base-v2 Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  AnnaWegmann/Style-Embedding Embeddings\n",
      "0       Formal Accuracy                                0.910505\n",
      "1      Complex Accuracy                                0.601717\n",
      "2  Contraction Accuracy                                0.946263\n",
      "3       Number Accuracy                                0.908687\n",
      "4      Average Accuracy                                0.841793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-uncased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-uncased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  sentence-transformers/all-mpnet-base-v2 Embeddings\n",
      "0       Formal Accuracy                                           0.881010 \n",
      "1      Complex Accuracy                                           0.564747 \n",
      "2  Contraction Accuracy                                           0.683838 \n",
      "3       Number Accuracy                                           0.767475 \n",
      "4      Average Accuracy                                           0.724268 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  google-bert/bert-base-uncased Embeddings\n",
      "0       Formal Accuracy                                  0.983838\n",
      "1      Complex Accuracy                                  0.702525\n",
      "2  Contraction Accuracy                                  0.909899\n",
      "3       Number Accuracy                                  0.985051\n",
      "4      Average Accuracy                                  0.895328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-multilingual-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-multilingual-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  google-bert/bert-base-cased Embeddings\n",
      "0       Formal Accuracy                                0.974545\n",
      "1      Complex Accuracy                                0.697071\n",
      "2  Contraction Accuracy                                0.943838\n",
      "3       Number Accuracy                                0.983232\n",
      "4      Average Accuracy                                0.899672\n",
      "                 Metric  google-bert/bert-base-multilingual-cased Embeddings\n",
      "0       Formal Accuracy                                           0.988687  \n",
      "1      Complex Accuracy                                           0.642323  \n",
      "2  Contraction Accuracy                                           0.944646  \n",
      "3       Number Accuracy                                           0.940404  \n",
      "4      Average Accuracy                                           0.879015  \n"
     ]
    }
   ],
   "source": [
    "STEL_table(\"AnnaWegmann/Style-Embedding\")\n",
    "STEL_table(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "STEL_table(\"google-bert/bert-base-uncased\")\n",
    "STEL_table(\"google-bert/bert-base-cased\")\n",
    "STEL_table(\"google-bert/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
