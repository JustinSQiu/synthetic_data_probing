{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Justin Qiu/Desktop/NLP Research/Synthetic Data Probing/project_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datadreamer import DataDreamer\n",
    "from datadreamer.llms import OpenAI\n",
    "from datadreamer.steps import DataFromPrompt, Embed, CosineSimilarity, concat, HFHubDataSource\n",
    "from datadreamer.embedders import SentenceTransformersEmbedder\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'Lexical Features' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "with DataDreamer(\"./output\"):\n",
    "    stel_dataset = HFHubDataSource(\n",
    "        \"Lexical Features\",\n",
    "        path=\"jjz5463/probing_dataset_4.0\",\n",
    "        split=\"train\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attributes': {'length': '10-20 words', 'point_of_view': 'second-person', 'sentence_type': 'Exclamation', 'tense': 'past', 'topic': 'Cloning a drive with CCC in block copy mode', 'voice': 'active voice'}, 'feature': 'formal', 'positive': 'You skillfully cloned a drive using CCC in block copy mode!', 'negative': 'Dude, you totally smashed cloning that drive with CCC in block copy mode!'}\n"
     ]
    }
   ],
   "source": [
    "print(stel_dataset.output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'Wegmann Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'Wegmann Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'BERT Feature Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'BERT Feature Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    }
   ],
   "source": [
    "with DataDreamer(\"./output\"):\n",
    "    wegmann_pos_embedded_data = Embed(\n",
    "        name = \"Wegmann Embeddings for Positive Examples\",\n",
    "        inputs = {\n",
    "            \"texts\": stel_dataset.output[\"positive\"]\n",
    "        },\n",
    "        args = {\n",
    "            \"embedder\": SentenceTransformersEmbedder(\n",
    "                model_name=\"AnnaWegmann/Style-Embedding\"\n",
    "            ),\n",
    "            \"truncate\": True\n",
    "        },\n",
    "        outputs = {\n",
    "            \"texts\": \"sentences\",\n",
    "            \"embeddings\": \"embeddings\"\n",
    "        },\n",
    "    )\n",
    "    wegmann_neg_embedded_data = Embed(\n",
    "        name = \"Wegmann Embeddings for Negative Examples\",\n",
    "        inputs = {\n",
    "            \"texts\": stel_dataset.output[\"negative\"]\n",
    "        },\n",
    "        args = {\n",
    "            \"embedder\": SentenceTransformersEmbedder(\n",
    "                model_name=\"AnnaWegmann/Style-Embedding\"\n",
    "            ),\n",
    "            \"truncate\": True\n",
    "        },\n",
    "        outputs = {\n",
    "            \"texts\": \"sentences\",\n",
    "            \"embeddings\": \"embeddings\"\n",
    "        },\n",
    "    )\n",
    "    bert_pos_embedded_data = Embed(\n",
    "        name = \"BERT Feature Embeddings for Positive Examples\",\n",
    "        inputs = {\n",
    "            \"texts\": stel_dataset.output[\"positive\"]\n",
    "        },\n",
    "        args = {\n",
    "            \"embedder\": SentenceTransformersEmbedder(\n",
    "                model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "            ),\n",
    "            \"truncate\": True\n",
    "        },\n",
    "        outputs = {\n",
    "            \"texts\": \"sentences\",\n",
    "            \"embeddings\": \"embeddings\"\n",
    "        },\n",
    "    )\n",
    "    bert_neg_embedded_data = Embed(\n",
    "        name = \"BERT Feature Embeddings for Negative Examples\",\n",
    "        inputs = {\n",
    "            \"texts\": stel_dataset.output[\"negative\"]\n",
    "        },\n",
    "        args = {\n",
    "            \"embedder\": SentenceTransformersEmbedder(\n",
    "                model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "            ),\n",
    "            \"truncate\": True\n",
    "        },\n",
    "        outputs = {\n",
    "            \"texts\": \"sentences\",\n",
    "            \"embeddings\": \"embeddings\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_formal_embeddings = np.array(wegmann_pos_embedded_data.output[\"embeddings\"][0:100])\n",
    "neg_formal_embeddings = np.array(wegmann_neg_embedded_data.output[\"embeddings\"][0:100])\n",
    "paired_formal_embeddings = [(pos, neg) for pos, neg in zip(pos_formal_embeddings, neg_formal_embeddings)]\n",
    "pos_complex_embeddings = np.array(wegmann_pos_embedded_data.output[\"embeddings\"][100:200])\n",
    "neg_complex_embeddings = np.array(wegmann_neg_embedded_data.output[\"embeddings\"][100:200])\n",
    "paired_complex_embeddings = [(pos, neg) for pos, neg in zip(pos_complex_embeddings, neg_complex_embeddings)]\n",
    "pos_contraction_embeddings = np.array(wegmann_pos_embedded_data.output[\"embeddings\"][200:300])\n",
    "neg_contraction_embeddings = np.array(wegmann_neg_embedded_data.output[\"embeddings\"][200:300])\n",
    "paired_contraction_embeddings = [(pos, neg) for pos, neg in zip(pos_contraction_embeddings, neg_contraction_embeddings)]\n",
    "pos_number_embeddings = np.array(wegmann_pos_embedded_data.output[\"embeddings\"][300:400])\n",
    "neg_number_embeddings = np.array(wegmann_neg_embedded_data.output[\"embeddings\"][300:400])\n",
    "paired_number_embeddings = [(pos, neg) for pos, neg in zip(pos_number_embeddings, neg_number_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formal embeddings accuracy with Wegmann: 0.9105050505050505\n",
      "4507 0 443\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(\n",
    "        paired_embeddings: list,\n",
    "):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        # anchor_pos = anchor_pos.reshape(1, -1)\n",
    "        # anchor_neg = anchor_neg.reshape(1, -1)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            # alt_pos = alt_pos.reshape(1, -1)\n",
    "            # alt_neg = alt_neg.reshape(1, -1)\n",
    "            \n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_neg, norm_alt_neg)\n",
    "            sim3 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            sim4 = np.dot(norm_anchor_neg, norm_alt_pos)\n",
    "            # sim1 = cosine_similarity(anchor_pos, alt_pos)[0][0]\n",
    "            # sim2 = cosine_similarity(anchor_neg, alt_neg)[0][0]\n",
    "            # sim3 = cosine_similarity(anchor_pos, alt_neg)[0][0]\n",
    "            # sim4 = cosine_similarity(anchor_neg, alt_pos)[0][0]\n",
    "            if math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) == math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) < math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_accuracy = 0\n",
    "correct = 0\n",
    "rand = 0\n",
    "incorrect = 0\n",
    "# max_val = float('-inf')\n",
    "# min_val = float('inf')\n",
    "for i in range(len(paired_formal_embeddings)):\n",
    "    anchor_pos, anchor_neg = paired_formal_embeddings[i]\n",
    "    norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "    # anchor_pos = anchor_pos.reshape(1, -1)\n",
    "    # anchor_neg = anchor_neg.reshape(1, -1)\n",
    "    for j in range(i+1, len(paired_formal_embeddings)):\n",
    "        alt_pos, alt_neg = paired_formal_embeddings[j]\n",
    "        norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "        # alt_pos = alt_pos.reshape(1, -1)\n",
    "        # alt_neg = alt_neg.reshape(1, -1)\n",
    "        \n",
    "        sim1 = np.dot(anchor_pos, alt_pos)\n",
    "        sim2 = np.dot(anchor_neg, alt_neg)\n",
    "        sim3 = np.dot(anchor_pos, alt_neg)\n",
    "        sim4 = np.dot(anchor_neg, alt_pos)\n",
    "        # print(sim1, sim2, sim3, sim4)\n",
    "        # max_val = max(max_val, sim1, sim2, sim3, sim4)\n",
    "        # min_val = min(min_val, sim1, sim2, sim3, sim4)\n",
    "        # sim1 = cosine_similarity(anchor_pos, alt_pos)[0][0]\n",
    "        # sim2 = cosine_similarity(anchor_neg, alt_neg)[0][0]\n",
    "        # sim3 = cosine_similarity(anchor_pos, alt_neg)[0][0]\n",
    "        # sim4 = cosine_similarity(anchor_neg, alt_pos)[0][0]\n",
    "        if math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) == math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "            formal_accuracy += 0.5\n",
    "            rand += 1\n",
    "        elif math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) < math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "            formal_accuracy += 1\n",
    "            correct += 1\n",
    "        else:\n",
    "            formal_accuracy += 0\n",
    "            incorrect += 1\n",
    "print(f\"Formal embeddings accuracy with Wegmann: {formal_accuracy / (len(paired_formal_embeddings) * (len(paired_formal_embeddings) - 1) / 2)}\")\n",
    "# print(max_val, min_val)\n",
    "print(correct, rand, incorrect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
