{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Justin Qiu/Desktop/NLP Research/Synthetic Data Probing/project_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datadreamer import DataDreamer\n",
    "from datadreamer.llms import OpenAI\n",
    "from datadreamer.steps import DataFromPrompt, Embed, CosineSimilarity, concat, HFHubDataSource\n",
    "from datadreamer.embedders import SentenceTransformersEmbedder\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'Lexical Features' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "with DataDreamer(\"./output\"):\n",
    "    stel_dataset = HFHubDataSource(\n",
    "        \"Lexical Features\",\n",
    "        path=\"jjz5463/probing_dataset_5.0\",\n",
    "        split=\"train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(\n",
    "        dataset_pos, dataset_neg, model: str\n",
    "):\n",
    "    with DataDreamer(\"./output\"):\n",
    "        pos_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Positive Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_pos\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "        neg_embedded_data = Embed(\n",
    "            name = f\"{model.replace('/', ' ')} Embeddings for Negative Examples\",\n",
    "            inputs = {\n",
    "                \"texts\": dataset_neg\n",
    "            },\n",
    "            args = {\n",
    "                \"embedder\": SentenceTransformersEmbedder(\n",
    "                    model_name=model\n",
    "                ),\n",
    "                \"truncate\": True\n",
    "            },\n",
    "            outputs = {\n",
    "                \"texts\": \"sentences\",\n",
    "                \"embeddings\": \"embeddings\"\n",
    "            },\n",
    "        )\n",
    "    return pos_embedded_data, neg_embedded_data\n",
    "\n",
    "def convert_embeddings(pos_embedded_data, neg_embedded_data):\n",
    "    pos_formal_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][0:100])\n",
    "    neg_formal_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][0:100])\n",
    "    paired_formal_embeddings = [(pos, neg) for pos, neg in zip(pos_formal_embeddings, neg_formal_embeddings)]\n",
    "    pos_complex_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][100:200])\n",
    "    neg_complex_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][100:200])\n",
    "    paired_complex_embeddings = [(pos, neg) for pos, neg in zip(pos_complex_embeddings, neg_complex_embeddings)]\n",
    "    pos_contraction_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][200:300])\n",
    "    neg_contraction_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][200:300])\n",
    "    paired_contraction_embeddings = [(pos, neg) for pos, neg in zip(pos_contraction_embeddings, neg_contraction_embeddings)]\n",
    "    pos_number_embeddings = np.array(pos_embedded_data.output[\"embeddings\"][300:400])\n",
    "    neg_number_embeddings = np.array(neg_embedded_data.output[\"embeddings\"][300:400])\n",
    "    paired_number_embeddings = [(pos, neg) for pos, neg in zip(pos_number_embeddings, neg_number_embeddings)]\n",
    "    return paired_formal_embeddings, paired_complex_embeddings, paired_contraction_embeddings, paired_number_embeddings\n",
    "\n",
    "def compute_accuracy(paired_embeddings: list):\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    rand = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(paired_embeddings)):\n",
    "        # print(f\"A1: {stel_dataset.output['positive'][i]}\")\n",
    "        # print(f\"A2: {stel_dataset.output['negative'][i]}\")\n",
    "        anchor_pos, anchor_neg = paired_embeddings[i]\n",
    "        norm_anchor_pos, norm_anchor_neg = anchor_pos / np.linalg.norm(anchor_pos), anchor_neg / np.linalg.norm(anchor_neg)\n",
    "        # anchor_pos = anchor_pos.reshape(1, -1)\n",
    "        # anchor_neg = anchor_neg.reshape(1, -1)\n",
    "        for j in range(i+1, len(paired_embeddings)):\n",
    "            # print(f\"S1 pair: {stel_dataset.output['positive'][j]}\")\n",
    "            # print(f\"S2 pair: {stel_dataset.output['negative'][j]}\")\n",
    "            alt_pos, alt_neg = paired_embeddings[j]\n",
    "            norm_alt_pos, norm_alt_neg = alt_pos / np.linalg.norm(alt_pos), alt_neg / np.linalg.norm(alt_neg)\n",
    "            # alt_pos = alt_pos.reshape(1, -1)\n",
    "            # alt_neg = alt_neg.reshape(1, -1)\n",
    "            \n",
    "            sim1 = np.dot(norm_anchor_pos, norm_alt_pos)\n",
    "            sim2 = np.dot(norm_anchor_neg, norm_alt_neg)\n",
    "            sim3 = np.dot(norm_anchor_pos, norm_alt_neg)\n",
    "            sim4 = np.dot(norm_anchor_neg, norm_alt_pos)\n",
    "            # print(f\"sim(A1, S1): {sim1}\")\n",
    "            # print(f\"sim(A2, S2): {sim2}\")\n",
    "            # print(f\"sim(A1, S2): {sim3}\")\n",
    "            # print(f\"sim(A2, S1): {sim4}\")\n",
    "            # print(f\"(1-sim1)^2 + (1-sim2)^2: {math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2)}\")\n",
    "            # print(f\"(1-sim3)^2 + (1-sim4)^2: {math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2)}\")\n",
    "            # print(f\"Prediction: {math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) < math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2)}\")\n",
    "            # sim1 = cosine_similarity(anchor_pos, alt_pos)[0][0]\n",
    "            # sim2 = cosine_similarity(anchor_neg, alt_neg)[0][0]\n",
    "            # sim3 = cosine_similarity(anchor_pos, alt_neg)[0][0]\n",
    "            # sim4 = cosine_similarity(anchor_neg, alt_pos)[0][0]\n",
    "            if math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) == math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 0.5\n",
    "                rand += 1\n",
    "            elif math.pow(1 - sim1, 2) + math.pow(1 - sim2, 2) < math.pow(1 - sim3, 2) + math.pow(1 - sim4, 2):\n",
    "                accuracy += 1\n",
    "                correct += 1\n",
    "            else:\n",
    "                accuracy += 0\n",
    "                incorrect += 1\n",
    "    return accuracy / (len(paired_embeddings) * (len(paired_embeddings) - 1) / 2)\n",
    "\n",
    "def STEL_benchmark(dataset_pos, dataset_neg, model):\n",
    "    pos_embedded_data, neg_embedded_data = compute_embeddings(dataset_pos, dataset_neg, model)\n",
    "    paired_formal_embeddings, paired_complex_embeddings, paired_contraction_embeddings, paired_number_embeddings = convert_embeddings(pos_embedded_data, neg_embedded_data)\n",
    "    formal_accuracy = compute_accuracy(paired_formal_embeddings)\n",
    "    complex_accuracy = compute_accuracy(paired_complex_embeddings)\n",
    "    contraction_accuracy = compute_accuracy(paired_contraction_embeddings)\n",
    "    number_accuracy = compute_accuracy(paired_number_embeddings)\n",
    "    avg_accuracy = (formal_accuracy + complex_accuracy + contraction_accuracy + number_accuracy) / 4\n",
    "    return formal_accuracy, complex_accuracy, contraction_accuracy, number_accuracy, avg_accuracy\n",
    "\n",
    "def STEL_print(model):\n",
    "    formal_accuracy, complex_accuracy, contraction_accuracy, number_accuracy, avg_accuracy = STEL_benchmark(stel_dataset.output[\"positive\"], stel_dataset.output[\"negative\"], model)\n",
    "    print(f\"Formal Accuracy for {model} Embeddings: {formal_accuracy}\")\n",
    "    print(f\"Complex Accuracy for {model} Embeddings: {complex_accuracy}\")\n",
    "    print(f\"Contraction Accuracy for {model} Embeddings: {contraction_accuracy}\")\n",
    "    print(f\"Number Accuracy for {model} Embeddings: {number_accuracy}\")\n",
    "    print(f\"Average Accuracy for {model} Embeddings: {avg_accuracy}\")\n",
    "\n",
    "def STEL_table(model):\n",
    "    formal_accuracy, complex_accuracy, contraction_accuracy, number_accuracy, avg_accuracy = STEL_benchmark(stel_dataset.output[\"positive\"], stel_dataset.output[\"negative\"], model)\n",
    "    data = {\n",
    "        'Metric': ['Formal Accuracy', 'Complex Accuracy', 'Contraction Accuracy', 'Number Accuracy', 'Average Accuracy'],\n",
    "        f'{model} Embeddings': [formal_accuracy, complex_accuracy, contraction_accuracy, number_accuracy, avg_accuracy]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'AnnaWegmann Style-Embedding Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'sentence-transformers all-mpnet-base-v2 Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'sentence-transformers all-mpnet-base-v2 Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  AnnaWegmann/Style-Embedding Embeddings\n",
      "0       Formal Accuracy                                0.885253\n",
      "1      Complex Accuracy                                0.721212\n",
      "2  Contraction Accuracy                                0.983636\n",
      "3       Number Accuracy                                0.973333\n",
      "4      Average Accuracy                                0.890859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-uncased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-uncased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  sentence-transformers/all-mpnet-base-v2 Embeddings\n",
      "0       Formal Accuracy                                           0.823232 \n",
      "1      Complex Accuracy                                           0.618384 \n",
      "2  Contraction Accuracy                                           0.878182 \n",
      "3       Number Accuracy                                           0.925859 \n",
      "4      Average Accuracy                                           0.811414 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  google-bert/bert-base-uncased Embeddings\n",
      "0       Formal Accuracy                                  0.973131\n",
      "1      Complex Accuracy                                  0.881818\n",
      "2  Contraction Accuracy                                  0.977980\n",
      "3       Number Accuracy                                  0.985859\n",
      "4      Average Accuracy                                  0.954697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: ./output\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-multilingual-cased Embeddings for Positive Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'google-bert bert-base-multilingual-cased Embeddings for Negative Examples' results loaded from disk. ðŸ™Œ It was previously run and saved.\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  google-bert/bert-base-cased Embeddings\n",
      "0       Formal Accuracy                                0.968081\n",
      "1      Complex Accuracy                                0.852929\n",
      "2  Contraction Accuracy                                0.986667\n",
      "3       Number Accuracy                                0.997374\n",
      "4      Average Accuracy                                0.951263\n",
      "                 Metric  google-bert/bert-base-multilingual-cased Embeddings\n",
      "0       Formal Accuracy                                           0.954949  \n",
      "1      Complex Accuracy                                           0.793333  \n",
      "2  Contraction Accuracy                                           0.997778  \n",
      "3       Number Accuracy                                           0.983636  \n",
      "4      Average Accuracy                                           0.932424  \n"
     ]
    }
   ],
   "source": [
    "STEL_table(\"AnnaWegmann/Style-Embedding\")\n",
    "STEL_table(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "STEL_table(\"google-bert/bert-base-uncased\")\n",
    "STEL_table(\"google-bert/bert-base-cased\")\n",
    "STEL_table(\"google-bert/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'to_add_const'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mSTEL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_style_models\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstyle_similarity\u001b[39;00m\n\u001b[1;32m      4\u001b[0m eval_style_models\u001b[38;5;241m.\u001b[39meval_sim(style_objects\u001b[38;5;241m=\u001b[39m[style_similarity\u001b[38;5;241m.\u001b[39mWordLengthSimilarity()])\n",
      "File \u001b[0;32m~/Desktop/NLP Research/Synthetic Data Probing/STEL/src/eval_style_models.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mto_add_const\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOCAL_STEL_DIM_QUAD, LOCAL_TOTAL_DIM_QUAD\n\u001b[1;32m     18\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutility\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfile_utility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_tsv_to_pd\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'to_add_const'"
     ]
    }
   ],
   "source": [
    "import STEL.src.eval_style_models\n",
    "import style_similarity\n",
    "\n",
    "eval_style_models.eval_sim(style_objects=[style_similarity.WordLengthSimilarity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
